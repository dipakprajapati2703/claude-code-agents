# Local Models Command

Display all available local models from Ollama and LM Studio.

## Usage
```
/local-models
```

## What it does
1. Discovers all Ollama models installed on your system
2. Checks for LM Studio models if server is running
3. Displays model information including:
   - Model name and size
   - Provider (Ollama/LM Studio)
   - Capabilities (code, chat, etc.)
   - Current routing assignments

## Output Example
```
ðŸ¤– Available Local Models:

Ollama Models:
  â€¢ llama2:7b (3.8GB) - General purpose
  â€¢ codellama:34b (19GB) - Code generation
  â€¢ mistral:latest (4.1GB) - Fast inference
  
LM Studio Models:
  â€¢ wizardcoder-15b (8.9GB) - Code specialist
  â€¢ phi-2 (1.6GB) - Small but capable

Current Routing:
  â€¢ Code Generation â†’ codellama:34b (Ollama)
  â€¢ Documentation â†’ llama2:7b (Ollama)
  â€¢ Testing â†’ wizardcoder-15b (LM Studio)

Total Models: 5
Total Size: 37.4GB
```

## Related Commands
- `/setup-ollama` - Install and configure Ollama
- `/setup-lmstudio` - Configure LM Studio
- `/configure-routing` - Customize which models handle which tasks
- `/models` - Show Claude models (built-in command)